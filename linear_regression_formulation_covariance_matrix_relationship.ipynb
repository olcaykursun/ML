{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Rdtn_9jlbg-9h9xMDQ2evLq0jvsQ2tx3",
      "authorship_tag": "ABX9TyPQqCzh7zE04bDW76sCgfVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olcaykursun/ML/blob/main/linear_regression_formulation_covariance_matrix_relationship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a summarized version of the steps to transition from the residual sum of squares (RSS) to the matrix form for multiple linear regression with two features $ x_1 $ and $ x_2 $:\n",
        "\n",
        "1. **RSS Formula**:\n",
        "$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_{i1} + w_2 x_{i2}))^2 $$\n",
        "\n",
        "2. **Expanding Squares**:\n",
        "Expand the square term in RSS to isolate each coefficient.\n",
        "$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i^2 - 2y_i(w_0 + w_1 x_{i1} + w_2 x_{i2}) + (w_0 + w_1 x_{i1} + w_2 x_{i2})^2 ) $$\n",
        "\n",
        "3. **Minimizing RSS**:\n",
        "Take the partial derivative of RSS with respect to each coefficient $ w_0, w_1, w_2 $, set it to zero, and solve the resulting equations. For example, for $ w_1 $:\n",
        "$$ \\frac{\\partial \\text{RSS}}{\\partial w_1} = -2 \\sum_{i=1}^{n} x_{i1}(y_i - (w_0 + w_1 x_{i1} + w_2 x_{i2})) = 0 $$\n",
        "\n",
        "4. **Matrix Form**:\n",
        "The term $ \\sum_{i=1}^{n} x_{i1}(y_i - (w_0 + w_1 x_{i1} + w_2 x_{i2})) $ can be represented in matrix form as $ \\mathbf{x}_1^T (\\mathbf{y} - \\mathbf{Xw}) $, leading to the normal equations:\n",
        "$$ \\mathbf{X}^T \\mathbf{X} \\mathbf{w} = \\mathbf{X}^T \\mathbf{y} $$\n",
        "\n",
        "5. **Solving for $ w $**:\n",
        "$$ \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
        "\n",
        "This sequence takes us from the RSS formula to the normal equations in matrix form and to the covariance matrix. It provides a compact way to represent and solve multiple linear regression."
      ],
      "metadata": {
        "id": "FUBP3gpZtrGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the term \"normal equations\" [1] refers to a system of equations that is derived by setting the gradient of the residual sum of squares (RSS) to zero. In the context of linear regression, this essentially means a closed-form solution for finding the weights that result in the smallest possible MSE (RSS) for a linear regression model.\n",
        "\n",
        "Also note that there needs to be a colum of \"1\"s added for the intercept term. The reason to include it in the RSS formula is that we are interested in finding the value of the intercept that minimizes the RSS, along with the values of the other coefficients. By including a column of ones, we can solve for all these terms simultaneously when we find the vector w that minimizes the RSS (the same technique is used in neural networks, see for example [2]).\n",
        "\n",
        "[1] https://mathworld.wolfram.com/NormalEquation.html\n",
        "\n",
        "[2] https://www.cmpe.boun.edu.tr/~ethem/i2ml3e/3e_v1-0/i2ml3e-chap11.pdf (Slides 5 and 6)"
      ],
      "metadata": {
        "id": "NHwuLinjxrmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/ML/\n",
        "!pwd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('grades_dataset.csv')\n",
        "df.head()\n",
        "\n",
        "X = df.drop(columns='Final_Grade').values\n",
        "y = df['Final_Grade'].values\n",
        "\n",
        "# Add a column of ones to X for the intercept term\n",
        "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "# Calculate the weights (coefficients) using the normal equation\n",
        "weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# The first element in 'weights' is the intercept, and the others are the coefficients\n",
        "print(f\"Intercept: {weights[0]}\")\n",
        "print(f\"Coefficients: {weights[1:]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1glDQfF8wamW",
        "outputId": "b36e0eed-7f84-4276-c00c-db7730654626"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/Colab Notebooks/ML\n",
            "/content/drive/MyDrive/Colab Notebooks/ML\n",
            "Intercept: 11.7450528577571\n",
            "Coefficients: [0.6889611  0.19356177]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_centered = X - np.mean(X, axis=0)\n",
        "N = X.shape[0]\n",
        "C = np.cov(X, rowvar=False, ddof=0)\n",
        "np.all(np.isclose(C,(X_centered.T @ X_centered)/N))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqBvVd8k18gl",
        "outputId": "906c28ed-e20f-4196-c3a0-414d8bc65386"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}