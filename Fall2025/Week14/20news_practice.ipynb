{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olcaykursun/ML/blob/main/Fall2025/Week14/20news_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz2iMGPWcIMJ"
      },
      "source": [
        "# Practice Notebook: 20-Newsgroups Classification\n",
        "This notebook contains Model A (Embeddings - similar to Week 13 notebook) and Model B (TF-IDF). Currently, they have pretty much the same accuracy. Which approach can you improve easier and how?"
      ],
      "id": "hz2iMGPWcIMJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKL166_cIML",
        "outputId": "e9cf4b89-c09f-45ec-d8d2-4d04117ef1ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Acc: 0.094\n",
            "Epoch 2, Train Acc: 0.197\n",
            "Epoch 3, Train Acc: 0.300\n",
            "Epoch 4, Train Acc: 0.411\n",
            "Epoch 5, Train Acc: 0.487\n",
            "Epoch 6, Train Acc: 0.555\n",
            "Epoch 7, Train Acc: 0.605\n",
            "Epoch 8, Train Acc: 0.637\n",
            "Epoch 9, Train Acc: 0.667\n",
            "Epoch 10, Train Acc: 0.689\n",
            "\n",
            "Embedding Model Test Accuracy: 0.6259285461620092\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Model A: Learned Word Embeddings\n",
        "# ==============================\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -------------------------\n",
        "# 0. Setup\n",
        "# -------------------------\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# -------------------------\n",
        "# 1. Load dataset\n",
        "# -------------------------\n",
        "data = fetch_20newsgroups(subset=\"all\", remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "texts = data.data\n",
        "labels = data.target\n",
        "num_classes = len(data.target_names)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Tokenizer (Keras)\n",
        "# -------------------------\n",
        "max_vocab = 20000\n",
        "tok = Tokenizer(num_words=max_vocab, oov_token=\"<UNK>\")\n",
        "tok.fit_on_texts(texts)\n",
        "seqs = tok.texts_to_sequences(texts)\n",
        "index_word = tok.index_word   # ID → word\n",
        "\n",
        "# -------------------------\n",
        "# 3. Remove stopwords\n",
        "# -------------------------\n",
        "def remove_sw(seq):\n",
        "    return [t for t in seq if index_word.get(t, \"\") not in stop_words]\n",
        "\n",
        "clean = [remove_sw(s) for s in seqs]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Trim + pad to first 250 tokens\n",
        "# -------------------------\n",
        "max_len = 250\n",
        "trimmed = [s[:max_len] for s in clean]\n",
        "\n",
        "X = np.zeros((len(trimmed), max_len), dtype=np.int64)\n",
        "for i, seq in enumerate(trimmed):\n",
        "    X[i, :len(seq)] = seq\n",
        "\n",
        "y = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 5. Dataset class\n",
        "# -------------------------\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "train_ds = NewsDataset(X_train, y_train)\n",
        "test_ds  = NewsDataset(X_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=64)\n",
        "\n",
        "# -------------------------\n",
        "# 6. Model: Embed → AvgPool → Classifier\n",
        "# -------------------------\n",
        "class AvgDocClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)        # (B, 250, dim)\n",
        "        avg = emb.mean(dim=1)      # average pooling\n",
        "        return self.fc(avg)\n",
        "\n",
        "vocab_size = min(max_vocab, len(tok.word_index) + 1)\n",
        "embed_dim = 64\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelA = AvgDocClassifier(vocab_size, embed_dim, num_classes).to(device)\n",
        "\n",
        "optimizer = optim.Adam(modelA.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# -------------------------\n",
        "# 7. Train 10 epochs\n",
        "# -------------------------\n",
        "for epoch in range(10):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    modelA.train()\n",
        "\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = modelA(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += len(yb)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Acc: {correct/total:.3f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 8. Evaluate\n",
        "# -------------------------\n",
        "modelA.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        preds = modelA(Xb).argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += len(yb)\n",
        "\n",
        "print(\"\\nEmbedding Model Test Accuracy:\", correct/total)\n"
      ],
      "id": "EhKL166_cIML"
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = tok.texts_to_sequences([\"this is a very rare wordadad\"])\n",
        "print(seqs)\n",
        "print(index_word[16])\n",
        "print(index_word[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsB9g89xd2g2",
        "outputId": "1eb29d09-ba44-4674-9b9b-0e879904f36e"
      },
      "id": "rsB9g89xd2g2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16, 10, 5, 112, 3541, 1]]\n",
            "this\n",
            "<UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caOeYxrUcIML",
        "outputId": "c0b63435-02ad-4ff7-d309-409e7afa6d5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Acc: 0.461\n",
            "Epoch 2, Train Acc: 0.658\n",
            "Epoch 3, Train Acc: 0.707\n",
            "Epoch 4, Train Acc: 0.739\n",
            "Epoch 5, Train Acc: 0.769\n",
            "\n",
            "TF-IDF Model Test Accuracy: 0.6204244031830238\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# Model B: TF-IDF + MLP\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ----------------------\n",
        "# 1. Load dataset\n",
        "# ----------------------\n",
        "data = fetch_20newsgroups(subset=\"all\", remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "texts = data.data\n",
        "labels = data.target\n",
        "num_classes = len(data.target_names)\n",
        "\n",
        "# ----------------------\n",
        "# 2. TF-IDF\n",
        "# ----------------------\n",
        "vec = TfidfVectorizer(max_features=2000, stop_words=\"english\")\n",
        "X = vec.fit_transform(texts).toarray()\n",
        "\n",
        "y = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=64)\n",
        "\n",
        "# ----------------------\n",
        "# 3. Tiny MLP\n",
        "# ----------------------\n",
        "class TfidfMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2000, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "modelB = TfidfMLP()\n",
        "optimizer = torch.optim.Adam(modelB.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ----------------------\n",
        "# 4. Train 5 epochs\n",
        "# ----------------------\n",
        "for epoch in range(5):\n",
        "    correct, total = 0, 0\n",
        "    for Xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = modelB(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += len(yb)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Acc: {correct/total:.3f}\")\n",
        "\n",
        "# ----------------------\n",
        "# 5. Evaluate\n",
        "# ----------------------\n",
        "modelB.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        preds = modelB(Xb).argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += len(yb)\n",
        "\n",
        "print(\"\\nTF-IDF Model Test Accuracy:\", correct/total)\n"
      ],
      "id": "caOeYxrUcIML"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can improve the embedding model much more easily, because TF-IDF is a fixed representation with limited expressiveness (it ignores context as it works as a bag-of-words, and therefore also ignores word order), whereas the embeddings approach can be upgraded with pretrained word embeddings and using CNNs, LSTMs, and Transformers to capture richer semantic and contextual information."
      ],
      "metadata": {
        "id": "rSYXkDYUe0Fu"
      },
      "id": "rSYXkDYUe0Fu"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2fAd6ajfqMc"
      },
      "id": "R2fAd6ajfqMc",
      "execution_count": null,
      "outputs": []
    }
  ]
}