{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olcaykursun/ML/blob/main/neuralnets/Simple_Data_Augmentation_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f24a3121-6702-4c85-a3d6-a416b6ca474e",
      "metadata": {
        "id": "f24a3121-6702-4c85-a3d6-a416b6ca474e"
      },
      "source": [
        "<b>Simple Data Augmentation with CNNs</b>  \n",
        "AUM Machine Learning, Dr. Olcay Kursun, okursun@aum.edu  \n",
        "Date: 04/11/2024 (Spring 2024)  \n",
        "\n",
        "Description: This script demonstrates the implementation of data augmentation, which enhances representation learning even with limited data. Data augmentation introduces diverse variations, thus helps prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6746f419-75e0-4ba8-b379-48a52b612e00",
      "metadata": {
        "id": "6746f419-75e0-4ba8-b379-48a52b612e00",
        "outputId": "1cc03e7f-f0bd-4b8d-d97d-475717c74cae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: Reshape and normalize\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Selecting a smaller subset for training to demonstrate how data augmentation can help\n",
        "subset_indices = np.random.choice(train_images.shape[0], 500, replace=False)\n",
        "train_images_subset = train_images[subset_indices]\n",
        "train_labels_subset = train_labels[subset_indices]\n",
        "\n",
        "# Function to display a grid of images\n",
        "def display_images(images, labels, num_rows=2, num_cols=5):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i in range(num_rows * num_cols):\n",
        "        plt.subplot(num_rows, num_cols, i + 1)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.title(f'Label: {labels[i]}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display the first few images and their labels\n",
        "display_images(train_images, train_labels.argmax(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b482d35-bb2b-4a0f-acd8-5ad5fde0a319",
      "metadata": {
        "id": "0b482d35-bb2b-4a0f-acd8-5ad5fde0a319"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada9e088-5263-481e-903d-2d4e792b5204",
      "metadata": {
        "id": "ada9e088-5263-481e-903d-2d4e792b5204",
        "outputId": "fd968f76-c505-427d-c81b-cd0a42db2caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-11 01:20:21.877117: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
            "2024-04-11 01:20:21.877142: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2024-04-11 01:20:21.877148: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2024-04-11 01:20:21.877200: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-04-11 01:20:21.877217: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1/16 [>.............................] - ETA: 5s - loss: 2.3144 - accuracy: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-11 01:20:22.264338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 1s 13ms/step - loss: 1.6328 - accuracy: 0.4560\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7690 - accuracy: 0.7480\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4185 - accuracy: 0.8740\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2626 - accuracy: 0.9180\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1391 - accuracy: 0.9540\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9700\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0532 - accuracy: 0.9880\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0334 - accuracy: 0.9940\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0182 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0123 - accuracy: 0.9980\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.5122e-04 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.6081e-04 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.0898e-04 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.3896e-04 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.9217e-04 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.2419e-04 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.6936e-04 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.4333e-04 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.0143e-04 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.6686e-04 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.3746e-04 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.1088e-04 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.8378e-04 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.8033e-04 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.6026e-04 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.2518e-04 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.1372e-04 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.0016e-04 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.7995e-04 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6332e-04 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4771e-04 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4196e-04 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.3241e-04 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.1528e-04 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.0611e-04 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.9842e-04 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.9043e-04 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.8665e-04 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7189e-04 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6751e-04 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6505e-04 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.5890e-04 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4653e-04 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4385e-04 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3873e-04 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3243e-04 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.2737e-04 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.2449e-04 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.1891e-04 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.1458e-04 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.1083e-04 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.0855e-04 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.0410e-04 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.0130e-04 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.7546e-05 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.4696e-05 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.2445e-05 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.8946e-05 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.6246e-05 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.3795e-05 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 8.2068e-05 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.9236e-05 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.6910e-05 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.4558e-05 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.2388e-05 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.0606e-05 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.8598e-05 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.6835e-05 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.5446e-05 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.3296e-05 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 6.1564e-05 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.9711e-05 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.8583e-05 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.7265e-05 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.5954e-05 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.4832e-05 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.3142e-05 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.1873e-05 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 5.0494e-05 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.9032e-05 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.8639e-05 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.6728e-05 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.5956e-05 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.4757e-05 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.3465e-05 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.2611e-05 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 4.1876e-05 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.0928e-05 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.0027e-05 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.9102e-05 - accuracy: 1.0000\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.8056 - accuracy: 0.8921\n",
            "test loss without augmentation: 0.8056265115737915\n",
            "test accuracy without augmentation: 0.8920999765396118\n"
          ]
        }
      ],
      "source": [
        "model_without_aug = create_model()\n",
        "model_without_aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_without_aug.fit(train_images_subset, train_labels_subset, epochs=100)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_without_aug.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"test loss without augmentation: {test_loss}\")\n",
        "print(f\"test accuracy without augmentation: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e88fe9f-83a5-4a70-9d5e-0820c559a786",
      "metadata": {
        "id": "9e88fe9f-83a5-4a70-9d5e-0820c559a786",
        "outputId": "4a4128db-eac2-4d6e-e26e-9b82ba9ca592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 2.1984 - accuracy: 0.2060\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6708 - accuracy: 0.4100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.2972 - accuracy: 0.5840\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.0981 - accuracy: 0.6440\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.8208 - accuracy: 0.7420\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7743 - accuracy: 0.7660\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5872 - accuracy: 0.8080\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5392 - accuracy: 0.8340\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5927 - accuracy: 0.8020\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4591 - accuracy: 0.8540\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3710 - accuracy: 0.8840\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3776 - accuracy: 0.8740\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3254 - accuracy: 0.8940\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3436 - accuracy: 0.8820\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2443 - accuracy: 0.9240\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3119 - accuracy: 0.9080\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2837 - accuracy: 0.9160\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2387 - accuracy: 0.9180\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3485 - accuracy: 0.8880\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2684 - accuracy: 0.9080\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2978 - accuracy: 0.9120\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2675 - accuracy: 0.9100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1885 - accuracy: 0.9400\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2723 - accuracy: 0.9120\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2489 - accuracy: 0.9140\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2127 - accuracy: 0.9320\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1692 - accuracy: 0.9320\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2610 - accuracy: 0.9160\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2726 - accuracy: 0.9220\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2935 - accuracy: 0.9120\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2294 - accuracy: 0.9400\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2356 - accuracy: 0.9260\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2594 - accuracy: 0.9120\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2294 - accuracy: 0.9220\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2313 - accuracy: 0.9280\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2144 - accuracy: 0.9380\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2006 - accuracy: 0.9340\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1569 - accuracy: 0.9380\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2845 - accuracy: 0.9300\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2958 - accuracy: 0.9280\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3238 - accuracy: 0.9160\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2537 - accuracy: 0.9240\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4425 - accuracy: 0.9140\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3573 - accuracy: 0.9200\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2574 - accuracy: 0.9300\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3017 - accuracy: 0.9300\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1963 - accuracy: 0.9380\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1514 - accuracy: 0.9440\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2427 - accuracy: 0.9460\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1596 - accuracy: 0.9580\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2548 - accuracy: 0.9380\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1797 - accuracy: 0.9380\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2499 - accuracy: 0.9300\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2244 - accuracy: 0.9340\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3718 - accuracy: 0.9320\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2109 - accuracy: 0.9340\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2181 - accuracy: 0.9600\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3399 - accuracy: 0.9340\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4966 - accuracy: 0.9020\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2804 - accuracy: 0.9500\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3084 - accuracy: 0.9400\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1638 - accuracy: 0.9560\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2625 - accuracy: 0.9400\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2627 - accuracy: 0.9420\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4571 - accuracy: 0.9120\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4047 - accuracy: 0.9260\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4840 - accuracy: 0.9240\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4299 - accuracy: 0.9280\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.8600 - accuracy: 0.8980\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7278 - accuracy: 0.8920\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5660 - accuracy: 0.9040\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5104 - accuracy: 0.9360\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2849 - accuracy: 0.9500\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5120 - accuracy: 0.9180\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7626 - accuracy: 0.9100\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6673 - accuracy: 0.9160\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.9776 - accuracy: 0.8960\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7817 - accuracy: 0.9240\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.7692 - accuracy: 0.9120\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.1527 - accuracy: 0.8960\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7959 - accuracy: 0.8840\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3722 - accuracy: 0.8820\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5972 - accuracy: 0.8900\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.2661 - accuracy: 0.8700\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.2443 - accuracy: 0.8820\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7110 - accuracy: 0.9060\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.4774 - accuracy: 0.8480\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.9499 - accuracy: 0.8940\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.1405 - accuracy: 0.8860\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.5127 - accuracy: 0.8720\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.1726 - accuracy: 0.9180\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.1856 - accuracy: 0.9140\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.0788 - accuracy: 0.8920\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.6671 - accuracy: 0.8480\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.0348 - accuracy: 0.8940\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.2393 - accuracy: 0.8900\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.8200 - accuracy: 0.8680\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.7437 - accuracy: 0.8880\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.8901 - accuracy: 0.8560\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 7.1582 - accuracy: 0.8760\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 5.8956 - accuracy: 0.9047\n",
            "test loss with augmentation: 5.895603656768799\n",
            "test accuracy with augmentation: 0.904699981212616\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "# Prepare the model\n",
        "model_with_aug = create_model()\n",
        "model_with_aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "augmented_data_generator = data_augmentation.flow(train_images_subset, train_labels_subset, batch_size=32)\n",
        "\n",
        "# Train the model with data augmentation\n",
        "model_with_aug.fit(augmented_data_generator, epochs=100)\n",
        "\n",
        "# Evaluate the target_model on the test set\n",
        "test_loss, test_acc = model_with_aug.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"test loss with augmentation: {test_loss}\")\n",
        "print(f\"test accuracy with augmentation: {test_acc}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf_metal)",
      "language": "python",
      "name": "tf_metal"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}