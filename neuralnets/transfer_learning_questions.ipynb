{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olcaykursun/ML/blob/main/neuralnets/transfer_learning_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26398df5-d3c4-447b-b4f9-a66d26ee5206",
      "metadata": {
        "id": "26398df5-d3c4-447b-b4f9-a66d26ee5206"
      },
      "source": [
        "Answer the following questions based on the incorrect code below:\n",
        "\n",
        "1. Fix Gross Errors in the Program:\n",
        "   - Identify and correct the issues in the program, such as unsuitable augmentation layers, incorrect loss function, incorrect number of color channels for MobileNetV2, and the missing use of preprocess_input for the pretrained model.\n",
        "   - The Sequential model is inherently meant to add layers one by one in order, either in a list or with the .add() method, whereas the Functional approach allows for more flexible graph-like connections, which doesnâ€™t fit well with the Sequential style. Fix it.\n",
        "\n",
        "2. Data Augmentation:\n",
        "   - Are the data augmentation layers appropriate for the MNIST dataset? Would the RandomFlip('vertical') augmentation make sense for digit classification?\n",
        "   - Given that MobileNetV2 was pretrained on ImageNet, does adding data augmentation layers help improve the performance of the model when applied to the MNIST dataset? Why or why not?\n",
        "\n",
        "3. Training:\n",
        "   - The base model's trainable attribute is set to True. Should the base model be trainable in this case, considering the goal of transfer learning? Why or why not?\n",
        "   - The final layers added to the model include a Dense(64, activation='relu') and a Dropout(0.2) layer. Is this sufficient for classifying the MNIST digits? Would you recommend different layers or settings to improve model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e62c1ad-5e86-4064-9e54-c9921263af0a",
      "metadata": {
        "id": "8e62c1ad-5e86-4064-9e54-c9921263af0a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2, preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "# There is a three-line spacing between code segments below, and each segment contains at least one bug to fix.\n",
        "\n",
        "\n",
        "\n",
        "# Load the MobileNetV2, which is a faster and more lightweight model, suitable for quicker training and inference.\n",
        "base = MobileNetV2(weights='imagenet', include_top=True, input_shape=(32, 32, 1))\n",
        "\n",
        "\n",
        "\n",
        "# Freeze the pretrained layers\n",
        "base.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "# Build the new model using Functional API incorrectly mixed with Sequential\n",
        "model = models.Sequential()\n",
        "input_tensor = Input(shape=(32, 32, 3))\n",
        "x = base(input_tensor)  # Using Functional approach\n",
        "# Add the MobileNetV2 base\n",
        "model.add(base(x))\n",
        "# Adding data augmentation layers to Sequential model\n",
        "model.add(layers.RandomFlip('horizontal'))\n",
        "model.add(layers.RandomFlip('vertical'))\n",
        "model.add(layers.RandomRotation(0.1))\n",
        "# Flatten and add output layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))  # Regularization to prevent overfitting\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Preprocess the data\n",
        "x_train = tf.image.resize(tf.expand_dims(x_train, axis=-1), [32, 32])\n",
        "x_test = tf.image.resize(tf.expand_dims(x_test, axis=-1), [32, 32])\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}