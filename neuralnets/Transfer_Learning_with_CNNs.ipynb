{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olcaykursun/ML/blob/main/neuralnets/Transfer_Learning_with_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ffbf9e-dea0-4563-8e10-bbf39397c767",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "15ffbf9e-dea0-4563-8e10-bbf39397c767"
      },
      "source": [
        "<b>Transfer Learning Implementation and Usage with CNNs</b>  \n",
        "AUM Machine Learning, Dr. Olcay Kursun, okursun@aum.edu  \n",
        "Date: 04/02/2024 (Spring 2024)  \n",
        "\n",
        "Description: This script demonstrates the implementation of a neural network model using TensorFlow and Keras. It covers the construction, compilation, and training of the model, along with examples of transfer learning and feature extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6ffbb2-1d8f-4cd4-8e70-ad3b1db3a148",
      "metadata": {
        "id": "8f6ffbb2-1d8f-4cd4-8e70-ad3b1db3a148",
        "outputId": "12cb8198-5a35-4100-a23a-7202b832db48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the full dataset\n",
        "(full_x_train, full_y_train), (full_x_test, full_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "full_x_train, full_x_test = full_x_train / 255.0, full_x_test / 255.0\n",
        "\n",
        "full_x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59f6341-8e7c-4c62-a41b-0641b240bba3",
      "metadata": {
        "id": "e59f6341-8e7c-4c62-a41b-0641b240bba3",
        "outputId": "f85e3a4f-d586-4552-9be7-4f01e3aeeaad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Expand dimensions to include channel information\n",
        "full_x_train = np.expand_dims(full_x_train, -1)\n",
        "full_x_test = np.expand_dims(full_x_test, -1)\n",
        "\n",
        "full_x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84973edb-96b9-42b6-994b-5a16ff6215e9",
      "metadata": {
        "id": "84973edb-96b9-42b6-994b-5a16ff6215e9",
        "outputId": "0be2c14f-ef2f-43f0-b247-a45df2a3e0f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36000, 6)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's say we keep the first n1 classes, build our features, and transfer that knowledge of how to extract features\n",
        "# to solve a different problem: A problem with few examples that belong to a totally new set of classes\n",
        "# for example if n1=8 then we remove classes 8 and 9 for this example and use classes 0-7 for learning good features\n",
        "n1 = 6        # We will use all the examples of these n1 classes: base task is to learn the base model\n",
        "n2 = 10 - n1  # We will use only 5 examples: target task is uses base to solve this new problem with a small dataset\n",
        "\n",
        "base_classes = range(n1)\n",
        "idx_train = np.isin(full_y_train, base_classes)\n",
        "idx_test = np.isin(full_y_test, base_classes)\n",
        "\n",
        "base_x_train, base_y_train = full_x_train[idx_train], full_y_train[idx_train]\n",
        "base_x_test, base_y_test = full_x_test[idx_test], full_y_test[idx_test]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "base_y_train = to_categorical(base_y_train, num_classes=n1)\n",
        "base_y_test = to_categorical(base_y_test, num_classes=n1)\n",
        "\n",
        "base_y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66d863e-6a00-4ffb-9da9-91aea427d253",
      "metadata": {
        "id": "b66d863e-6a00-4ffb-9da9-91aea427d253",
        "outputId": "c3f358c5-c34e-4f2b-a649-7bcecd3fe1df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 4)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will test the quality of the features learned on the n2 classes with few examples\n",
        "num_samples_per_class = 5\n",
        "\n",
        "target_classes = range(n1, 10)\n",
        "idx_train = np.isin(full_y_train, target_classes)\n",
        "idx_test = np.isin(full_y_test, target_classes)\n",
        "\n",
        "target_x_train, target_y_train = full_x_train[idx_train], full_y_train[idx_train]\n",
        "\n",
        "all_selected_indices = []\n",
        "for class_label in target_classes:\n",
        "    # Indices of all instances of the current class\n",
        "    class_indices = np.where(target_y_train == class_label)[0]\n",
        "\n",
        "    # Randomly choose num_samples_per_class indices from this class\n",
        "    chosen_indices = np.random.choice(class_indices, num_samples_per_class, replace=False)\n",
        "\n",
        "    # Append these indices to the list\n",
        "    all_selected_indices.extend(chosen_indices)  # Use extend or +\n",
        "\n",
        "# Use the selected indices to build the small dataset for the target task\n",
        "target_x_train, target_y_train = target_x_train[all_selected_indices], target_y_train[all_selected_indices]\n",
        "\n",
        "target_x_test, target_y_test = full_x_test[idx_test], full_y_test[idx_test]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "target_y_train = to_categorical(target_y_train - n1, num_classes=n2)\n",
        "target_y_test = to_categorical(target_y_test - n1, num_classes=n2)\n",
        "# For binary (class 8 vs 9), we could also use:\n",
        "#target_y_train = 0+(target_y_train == 8)   #also works: (target_y_train == 8).astype(int)\n",
        "\n",
        "target_y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f4851c-db39-4530-ace1-9ac691054929",
      "metadata": {
        "id": "02f4851c-db39-4530-ace1-9ac691054929",
        "outputId": "c6a1c8fb-d5a1-44fd-872f-60c9367d8e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ True, False, False, ..., False, False, False])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8321c7b-aee1-4168-a17d-dd64480b69d0",
      "metadata": {
        "id": "e8321c7b-aee1-4168-a17d-dd64480b69d0",
        "outputId": "c502949f-e4db-482c-d7b2-e0c95efc3203"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 28, 28, 1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0a242c-4551-476c-8b02-ef2f3f7f3b16",
      "metadata": {
        "id": "9c0a242c-4551-476c-8b02-ef2f3f7f3b16",
        "outputId": "9b9099fb-7826-4886-f671-ac91e9cc860c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 03:16:51.266975: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
            "2024-04-03 03:16:51.266996: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2024-04-03 03:16:51.267002: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2024-04-03 03:16:51.267032: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-04-03 03:16:51.267045: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n1, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d814563-83b8-4608-a192-fe5ac37e983f",
      "metadata": {
        "id": "3d814563-83b8-4608-a192-fe5ac37e983f",
        "outputId": "b9c0a194-bbc1-4ca1-a5a6-6b83fb932d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                102464    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 121670 (475.27 KB)\n",
            "Trainable params: 121670 (475.27 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2840579-71c0-4e2c-8cc0-7645197602c6",
      "metadata": {
        "id": "a2840579-71c0-4e2c-8cc0-7645197602c6",
        "outputId": "344e2f86-f422-497d-f5e5-de50bcdf0d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  1/900 [..............................] - ETA: 4:20 - loss: 1.7915 - accuracy: 0.2188"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 03:16:51.639028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "900/900 [==============================] - 9s 10ms/step - loss: 0.3390 - accuracy: 0.8756 - val_loss: 0.2331 - val_accuracy: 0.9196\n",
            "Epoch 2/5\n",
            "900/900 [==============================] - 9s 10ms/step - loss: 0.2188 - accuracy: 0.9225 - val_loss: 0.2126 - val_accuracy: 0.9239\n",
            "Epoch 3/5\n",
            "900/900 [==============================] - 9s 10ms/step - loss: 0.1877 - accuracy: 0.9346 - val_loss: 0.2066 - val_accuracy: 0.9265\n",
            "Epoch 4/5\n",
            "900/900 [==============================] - 9s 10ms/step - loss: 0.1723 - accuracy: 0.9402 - val_loss: 0.2001 - val_accuracy: 0.9324\n",
            "Epoch 5/5\n",
            "900/900 [==============================] - 9s 10ms/step - loss: 0.1570 - accuracy: 0.9456 - val_loss: 0.1873 - val_accuracy: 0.9369\n",
            "\n",
            "Not the focus of transfer learning, but a good accuracy here shows learned base features can be good:\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 0.1806 - accuracy: 0.9392\n",
            "Base-task test loss: 0.18056409060955048\n",
            "Base-task test accuracy: 0.9391666650772095\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Alternative way of computing accuracy: 0.9391666666666667\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(base_x_train, base_y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Evaluate the new model on the test set\n",
        "print('\\nNot the focus of transfer learning, but a good accuracy here shows learned base features can be good:')\n",
        "test_loss, test_acc = model.evaluate(base_x_test, base_y_test)\n",
        "print(f\"Base-task test loss: {test_loss}\")\n",
        "print(f\"Base-task test accuracy: {test_acc}\")\n",
        "\n",
        "# Alternative to \"evaluate\", we can use the model to predict the classes of the test set and then calculate the accuracy\n",
        "predictions = model.predict(base_x_test)\n",
        "# The predictions are in the form of probabilities for each class.\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(base_y_test, axis=1)\n",
        "# Calculate accuracy by comparing predicted and true classes\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Alternative way of computing accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acfd57d3-5097-4fa9-b771-a7c359544a19",
      "metadata": {
        "id": "acfd57d3-5097-4fa9-b771-a7c359544a19",
        "outputId": "fc34a112-e13c-408a-f7fc-886dbc1b451f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "125/125 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Extract features\n",
        "# We can feed them to Decision Tree (instead of the dense layers forming an MLP in the original setup)\n",
        "# We can feed them to a clustering algorithm as inputs for clustering never before seen examples and classes\n",
        "\n",
        "# remove the MLP from the top (the two dense layers)\n",
        "feature_extractor = models.Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "features_target_train = feature_extractor.predict(target_x_train) #get features for the training set of the target task\n",
        "features_target_test = feature_extractor.predict(target_x_test) #get features for the test set of the target task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c64557-409b-429f-848c-6fe16c4df1e1",
      "metadata": {
        "id": "66c64557-409b-429f-848c-6fe16c4df1e1",
        "outputId": "6a4ae30c-f709-463e-96ac-f58c52856733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target-task Decision Tree Accuracy (using features learned from the base): 0.7375\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train the decision tree using the features learned from the base task\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(features_target_train, np.argmax(target_y_train, axis=1))  # Fit to the max class index\n",
        "\n",
        "# Evaluate the decision tree\n",
        "accuracy = tree.score(features_target_test, np.argmax(target_y_test, axis=1))\n",
        "print(f\"Target-task Decision Tree Accuracy (using features learned from the base): {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e44e67-69da-4d35-9556-f9034a931479",
      "metadata": {
        "id": "a4e44e67-69da-4d35-9556-f9034a931479",
        "outputId": "6aea2921-a4b4-4d4d-8d84-f85833db0b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target-task Decision Tree Accuracy using raw features: 0.6865\n"
          ]
        }
      ],
      "source": [
        "# Use the raw features for training/testing the decision tree\n",
        "\n",
        "num_input_channels = np.prod(input_shape)\n",
        "\n",
        "# Instantiate a new decision tree and train on the small dataset without knowledge-transfer (from scratch)\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(target_x_train.reshape(-1, num_input_channels), np.argmax(target_y_train, axis=1))  # Fit to the max class index\n",
        "\n",
        "# Evaluate the decision tree\n",
        "accuracy = tree.score(target_x_test.reshape(-1, num_input_channels), np.argmax(target_y_test, axis=1))\n",
        "print(f\"Target-task Decision Tree Accuracy using raw features: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762b853c-49eb-4ea4-88f0-609c294f73a9",
      "metadata": {
        "id": "762b853c-49eb-4ea4-88f0-609c294f73a9",
        "outputId": "5205537b-4adf-4532-9c65-1d991f22451e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 1.4517 - accuracy: 0.3000\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1980 - accuracy: 0.6500\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9948 - accuracy: 0.8000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8262 - accuracy: 0.9500\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6781 - accuracy: 0.9500\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7997 - accuracy: 0.8202\n",
            "Standalone MLP Test loss: 0.7997000813484192\n",
            "Standalone MLP Test accuracy: 0.8202499747276306\n"
          ]
        }
      ],
      "source": [
        "# Learned features can be used as input to a standalone MLP as well\n",
        "\n",
        "# New model using the extracted features for n2-class classification problem\n",
        "new_model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(features_target_train.shape[1],)),\n",
        "    layers.Dense(n2, activation='softmax')  #For n2=1 can also be replaced by (1, 'sigmoid') and binary_crossentropy\n",
        "])\n",
        "\n",
        "# Train the new model on the target-task\n",
        "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "new_model.fit(features_target_train, target_y_train, epochs=5)\n",
        "\n",
        "# Evaluate the new model on the test set\n",
        "test_loss, test_acc = new_model.evaluate(features_target_test, target_y_test)\n",
        "\n",
        "print(f\"Standalone MLP Test loss: {test_loss}\")\n",
        "print(f\"Standalone MLP Test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe5d9e6-cbbf-4cd6-9886-587915b2f3f5",
      "metadata": {
        "id": "7fe5d9e6-cbbf-4cd6-9886-587915b2f3f5",
        "outputId": "af08957b-f503-4a42-984a-9720d63647fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 1.4541 - accuracy: 0.2500\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1846 - accuracy: 0.6500\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9717 - accuracy: 0.7500\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7862 - accuracy: 0.9500\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6287 - accuracy: 0.9500\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7466 - accuracy: 0.8570\n",
            "target_model test loss: 0.7466206550598145\n",
            "target_model test accuracy: 0.8569999933242798\n"
          ]
        }
      ],
      "source": [
        "# Start fine-tuning\n",
        "\n",
        "# Create a new classifier-layer on top use unpacking of the layers of the model, shallow copied\n",
        "\n",
        "# This will perform shallow copy of the base layers\n",
        "# So, if there were complex connections in the base, then this should be used as it preserves the architecture\n",
        "# as opposed to the unpacking method below\n",
        "\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# In Keras, layers and models (including submodels created with the Model constructor) are all callable objects.\n",
        "# This means you can treat them like functions and pass input tensors to them to obtain output tensors.\n",
        "truncated_pretrained_model = models.Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "target_model = models.Sequential([\n",
        "    truncated_pretrained_model,         # Use the submodel/functional at first, again a form of shallow copy of the layers\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Freeze all layers coming from the base model\n",
        "for layer in target_model.layers[:-2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "target_model.fit(target_x_train, target_y_train, epochs=5)\n",
        "# Evaluate the target_model on the test set\n",
        "test_loss, test_acc = target_model.evaluate(target_x_test, target_y_test)\n",
        "\n",
        "print(f\"target_model test loss: {test_loss}\")\n",
        "print(f\"target_model test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7f17bf-84f4-434f-ac2a-d65607087a42",
      "metadata": {
        "id": "5d7f17bf-84f4-434f-ac2a-d65607087a42",
        "outputId": "1bc82289-8197-4563-83bf-12dd19f56b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4986 - accuracy: 0.9500\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3518 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2392 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1555 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0970 - accuracy: 1.0000\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.3539 - accuracy: 0.9020\n",
            "Retrained target_model test loss: 0.35391634702682495\n",
            "Retrained target_model test accuracy: 0.9020000100135803\n",
            "False\n",
            "Weights of the first layers are the same.\n"
          ]
        }
      ],
      "source": [
        "# Unfreeze all layers in the target_model, not recommended for small datasets\n",
        "# Due to the shallow copy above, the base model will also change\n",
        "for layer in target_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model, because it will initialize the computation graph that contains learning rates, momentums etc.\n",
        "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "target_model.fit(target_x_train, target_y_train, epochs=5)\n",
        "\n",
        "# Evaluate the target_model on the test set\n",
        "test_loss, test_acc = target_model.evaluate(target_x_test, target_y_test)\n",
        "\n",
        "print(f\"Retrained target_model test loss: {test_loss}\")\n",
        "print(f\"Retrained target_model test accuracy: {test_acc}\")\n",
        "\n",
        "print(target_model.layers[0]==model.layers[0])\n",
        "weights_model = model.layers[0].get_weights()\n",
        "weights_target_model = target_model.layers[0].get_weights()\n",
        "if np.array_equal(weights_model[1], weights_target_model[1]):\n",
        "    print(\"Weights of the first layers are the same.\")\n",
        "else:\n",
        "    print(\"Weights of the first layers are different.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12487d75-3d57-454d-8101-049de4134532",
      "metadata": {
        "id": "12487d75-3d57-454d-8101-049de4134532",
        "outputId": "2f8cb123-4639-4060-c919-19066ce92858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 1.3599 - accuracy: 0.4000\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0367 - accuracy: 0.6000\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8118 - accuracy: 0.8500\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6183 - accuracy: 0.9000\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4688 - accuracy: 0.9500\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5887 - accuracy: 0.8665\n",
            "target_model test loss: 0.5886551141738892\n",
            "target_model test accuracy: 0.8665000200271606\n"
          ]
        }
      ],
      "source": [
        "# Unpacking approach is not preferred\n",
        "\n",
        "target_model = models.Sequential([\n",
        "    *model.layers[:-2],\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Freeze all layers coming from the base model\n",
        "for layer in target_model.layers[:-2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "target_model.fit(target_x_train, target_y_train, epochs=5)\n",
        "# Evaluate the target_model on the test set\n",
        "test_loss, test_acc = target_model.evaluate(target_x_test, target_y_test)\n",
        "\n",
        "print(f\"target_model test loss: {test_loss}\")\n",
        "print(f\"target_model test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9043602f-082b-4e5c-ab65-f5fede7f9015",
      "metadata": {
        "id": "9043602f-082b-4e5c-ab65-f5fede7f9015",
        "outputId": "a4ef1cbd-280c-4c11-c5d3-91e418199daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6410 - accuracy: 0.8568\n",
            "target_model test loss: 0.641007661819458\n",
            "target_model test accuracy: 0.8567500114440918\n"
          ]
        }
      ],
      "source": [
        "# Copy options: saving/loading the model, cloning, and serialization/deserialization.\n",
        "\n",
        "from tensorflow.keras.models import clone_model, Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "cloned_model = clone_model(model)\n",
        "cloned_model.set_weights(model.get_weights())\n",
        "\n",
        "model_input = Input(shape=input_shape)\n",
        "x = model_input\n",
        "for layer in cloned_model.layers[:-2]:\n",
        "    x = layer(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "new_output = Dense(n2, activation='softmax')(x)\n",
        "target_model = Model(inputs=model_input, outputs=new_output)\n",
        "\n",
        "for layer in target_model.layers[:-2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "target_model.fit(target_x_train, target_y_train, epochs=5, verbose=0)\n",
        "test_loss, test_acc = target_model.evaluate(target_x_test, target_y_test)\n",
        "print(f\"target_model test loss: {test_loss}\")\n",
        "print(f\"target_model test accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6faa212-d71e-42b7-be24-edbb1a13da92",
      "metadata": {
        "id": "f6faa212-d71e-42b7-be24-edbb1a13da92",
        "outputId": "48dfe969-19c5-4c84-a123-9628961c24b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5346 - accuracy: 0.8745\n",
            "target_model test loss: 0.5345913767814636\n",
            "target_model test accuracy: 0.8744999766349792\n"
          ]
        }
      ],
      "source": [
        "# In a way, cloning approaches work like this:\n",
        "# Manually replicate the architecture of the original model and copy the weights of corresponding layers.\n",
        "\n",
        "target_model = models.Sequential([\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n2, activation='softmax')\n",
        "])\n",
        "\n",
        "for i, layer in enumerate(model.layers[:-2]):\n",
        "    target_model.layers[i].set_weights(layer.get_weights())\n",
        "    target_model.layers[i].trainable = False\n",
        "\n",
        "target_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "target_model.fit(target_x_train, target_y_train, epochs=5, verbose=0)\n",
        "test_loss, test_acc = target_model.evaluate(target_x_test, target_y_test)\n",
        "print(f\"target_model test loss: {test_loss}\")\n",
        "print(f\"target_model test accuracy: {test_acc}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf_metal)",
      "language": "python",
      "name": "tf_metal"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}